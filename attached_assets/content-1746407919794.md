closeclose

[iframe](https://content.googleapis.com/static/proxy.html?usegapi=1&jsh=m%3B%2F_%2Fscs%2Fabc-static%2F_%2Fjs%2Fk%3Dgapi.gapi.en.F939Du45chc.O%2Fd%3D1%2Frs%3DAHpOoo8uI5v7Xlp-b-Z4Th_hAAVtm2lZOw%2Fm%3D__features__#parent=https%3A%2F%2Fcolab.research.google.com&rpctoken=388494566)

info

This notebook is open with private outputs. Outputs will not be saved. You can disable this in [Notebook settings](https://colab.research.google.com/drive/1KhXT56UePPUHhqitJNUxq63k-pQomz3N#).


close

Commands
Code
Text
Copy to Drive
peoplesettingsexpand\_lessexpand\_more

format\_list\_bulleted

find\_in\_page

code

vpn\_key

folder

terminal

Notebook

more\_horiz

* * *

sparkGemini


model\_name = "canopylabs/orpheus-3b-0.1-ft"

# change to appropriate model for different voices/languages

# https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba

print("\*\*\* Change the model you use here")

```
*** Change the model you use here

```

* * *

sparkGemini


keyboard\_arrow\_down

### Installation & Setup

```
#@title Installation & Setup
%%capture
!pip install snac ipywebrtc
from snac import SNAC
import torch
import torch
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer
import numpy as np
import soundfile as sf
import IPython.display as ipd
import librosa
from ipywebrtc import AudioRecorder, Audio
from IPython.display import display
import ipywidgets as widgets

snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz")
snac_model = snac_model.to("cpu")

print("We have loaded the tokeniser/detokeniser model to the cpu, to use vram - use the gpu for faster inference")

tokeniser_name = "meta-llama/Llama-3.2-3B-Instruct"
from huggingface_hub import snapshot_download

# Download only model config and safetensors
model_path = snapshot_download(
    repo_id=model_name,
    allow_patterns=[\
        "config.json",\
        "*.safetensors",\
        "model.safetensors.index.json",\
    ],
    ignore_patterns=[\
        "optimizer.pt",\
        "pytorch_model.bin",\
        "training_args.bin",\
        "scheduler.pt",\
        "tokenizer.json",\
        "tokenizer_config.json",\
        "special_tokens_map.json",\
        "vocab.json",\
        "merges.txt",\
        "tokenizer.*"\
    ]
)

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)
model.cuda()
tokenizer = AutoTokenizer.from_pretrained(model_name)

```

Show code

* * *

sparkGemini


#### CHANGE THIS ####

prompts = \[\
\
"Hey there my name is Tara, <chuckle> and I'm a speech generation model that can sound like a person.",\
\
"I've also been taught to understand and produce paralinguistic things like sighing, or chuckling, or yawning!",\
\
"I live in San Francisco, and have, uhm let's see, 3 billion 7 hundred ... well, lets just say a lot of parameters.",\
\
\]

chosen\_voice = "tara"# see github for other voices

print("\*\*\* See our github for tips on prompting the model for cleaning, humanlike generations.")

```
*** See our github for tips on prompting the model for cleaning, humanlike generations.

```

* * *

sparkGemini


keyboard\_arrow\_down

### Format prompts into correct template

```
#@title Format prompts into correct template

prompts = [f"{chosen_voice}: " + p for p in prompts]

all_input_ids = []

for prompt in prompts:
  input_ids = tokenizer(prompt, return_tensors="pt").input_ids
  all_input_ids.append(input_ids)

start_token = torch.tensor([[ 128259]], dtype=torch.int64) # Start of human
end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64) # End of text, End of human

all_modified_input_ids = []
for input_ids in all_input_ids:
  modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1) # SOH SOT Text EOT EOH
  all_modified_input_ids.append(modified_input_ids)

all_padded_tensors = []
all_attention_masks = []
max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])
for modified_input_ids in all_modified_input_ids:
  padding = max_length - modified_input_ids.shape[1]
  padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)
  attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)
  all_padded_tensors.append(padded_tensor)
  all_attention_masks.append(attention_mask)

all_padded_tensors = torch.cat(all_padded_tensors, dim=0)
all_attention_masks = torch.cat(all_attention_masks, dim=0)

input_ids = all_padded_tensors.to("cuda")
attention_mask = all_attention_masks.to("cuda")

```

Show code

* * *

sparkGemini


keyboard\_arrow\_down

### Generate Output

```
#@title Generate Output
print("*** Model.generate is slow - see vllm implementation on github for realtime streaming and inference")
print("*** Increase/decrease inference params for more expressive less stable generations")

with torch.no_grad():
  generated_ids = model.generate(
      input_ids=input_ids,
      attention_mask=attention_mask,
      max_new_tokens=1200,
      do_sample=True,
      temperature=0.6,
      top_p=0.95,
      repetition_penalty=1.1,
      num_return_sequences=1,
      eos_token_id=128258,
  )

```

Show code

```
*** Model.generate is slow - see vllm implementation on github for realtime streaming and inference
*** Increase/decrease inference params for more expressive less stable generations

```

* * *

sparkGemini


keyboard\_arrow\_down

### Parse Output as speech

```
#@title Parse Output as speech
token_to_find = 128257
token_to_remove = 128258

token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)

if len(token_indices[1]) > 0:
    last_occurrence_idx = token_indices[1][-1].item()
    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]
else:
    cropped_tensor = generated_ids

mask = cropped_tensor != token_to_remove

processed_rows = []

for row in cropped_tensor:
    masked_row = row[row != token_to_remove]
    processed_rows.append(masked_row)

code_lists = []

for row in processed_rows:
    row_length = row.size(0)
    new_length = (row_length // 7) * 7
    trimmed_row = row[:new_length]
    trimmed_row = [t - 128266 for t in trimmed_row]
    code_lists.append(trimmed_row)

def redistribute_codes(code_list):
  layer_1 = []
  layer_2 = []
  layer_3 = []
  for i in range((len(code_list)+1)//7):
    layer_1.append(code_list[7*i])
    layer_2.append(code_list[7*i+1]-4096)
    layer_3.append(code_list[7*i+2]-(2*4096))
    layer_3.append(code_list[7*i+3]-(3*4096))
    layer_2.append(code_list[7*i+4]-(4*4096))
    layer_3.append(code_list[7*i+5]-(5*4096))
    layer_3.append(code_list[7*i+6]-(6*4096))
  codes = [torch.tensor(layer_1).unsqueeze(0),\
         torch.tensor(layer_2).unsqueeze(0),\
         torch.tensor(layer_3).unsqueeze(0)]
  audio_hat = snac_model.decode(codes)
  return audio_hat

my_samples = []
for code_list in code_lists:
  samples = redistribute_codes(code_list)
  my_samples.append(samples)

```

Show code

* * *

sparkGemini


keyboard\_arrow\_down

### Display Audio

```
#@title Display Audio
from IPython.display import display, Audio
if len(prompts) != len(my_samples):
  raise Exception("Number of prompts and samples do not match")
else:
  for i in range(len(my_samples)):
    print(prompts[i])
    samples = my_samples[i]
    display(Audio(samples.detach().squeeze().to("cpu").numpy(), rate=24000))

```

Show code

[iframe](https://scj82y1qew-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20250430-060127_RC00_753083549)

* * *

sparkGemini


* * *

[Colab paid products](https://colab.research.google.com/signup?utm_source=footer&utm_medium=link&utm_campaign=footer_links)
 -
[Cancel contracts here](https://colab.research.google.com/cancel-subscription)

more\_horiz

more\_horiz

more\_horiz

[iframe](https://scj82y1qew-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20250430-060127_RC00_753083549)

[iframe](https://63ie4ilyowl-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20250430-060127_RC00_753083549)

Locate in Drive

New notebook in Drive

Open notebook

Upload notebook

Rename

Move

Move to trash

Save a copy in Drive

Save a copy as a GitHub Gist

Save a copy in GitHub

Save

Save and pin revision

Revision history

Download
►

Print

Download .ipynb

Download .py

Undo

Redo

Select all cells

Cut cell or selection

Copy cell or selection

Paste

Delete selected cells

Find and replace

Find next

Find previous

Notebook settings

Clear all outputs

check

Table of contents

Notebook info

Executed code history

Comments
►

Collapse sections

Expand sections

Save collapsed section layout

Show/hide code

Show/hide output

Focus next tab

Focus previous tab

Move tab to next pane

Move tab to previous pane

Hide comments

Minimize comments

Expand comments

Code cell

Text cell

Section header cell

Scratch code cell

Code snippets

Add a form field

Run all

Run before

Run the focused cell

Run selection

Run cell and below

Interrupt execution

Restart session

Restart session and run all

Disconnect and delete runtime

Change runtime type

Manage sessions

View resources

View runtime logs

Command palette

Settings

Keyboard shortcuts

Diff notebooks(opens in a new tab)

Frequently asked questions

View release notes

Search code snippets

Report a bug

Report Drive abuse

Send feedback

View terms of service

[iframe](/_/bscframe)

[iframe](https://www.google.com/recaptcha/api2/anchor?ar=1&k=6LfQPtEUAAAAAHBpAdFng54jyuB1V5w5dofknpip&co=aHR0cHM6Ly9jb2xhYi5yZXNlYXJjaC5nb29nbGUuY29tOjQ0Mw..&hl=en&v=w0_qmZVSdobukXrBwYd9dTF7&size=invisible&cb=9mmy78c4ai9z)