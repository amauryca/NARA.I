[Skip to content](https://github.com/remsky/kokoro-fastapi#start-of-content)

You signed in with another tab or window. [Reload](https://github.com/remsky/kokoro-fastapi) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/remsky/kokoro-fastapi) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/remsky/kokoro-fastapi) to refresh your session.Dismiss alert

[remsky](https://github.com/remsky)/ **[Kokoro-FastAPI](https://github.com/remsky/Kokoro-FastAPI)** Public

- Sponsor







# Sponsor remsky/Kokoro-FastAPI



















##### External links





![buy_me_a_coffee](https://github.githubassets.com/assets/buy_me_a_coffee-63ed78263f6e.svg)



[buymeacoffee.com/ **remsky**](https://buymeacoffee.com/remsky)









[Learn more about funding links in repositories](https://docs.github.com/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/displaying-a-sponsor-button-in-your-repository).




[Report abuse](https://github.com/contact/report-abuse?report=remsky%2FKokoro-FastAPI+%28Repository+Funding+Links%29)

- [Notifications](https://github.com/login?return_to=%2Fremsky%2FKokoro-FastAPI) You must be signed in to change notification settings
- [Fork\\
353](https://github.com/login?return_to=%2Fremsky%2FKokoro-FastAPI)
- [Star\\
2.5k](https://github.com/login?return_to=%2Fremsky%2FKokoro-FastAPI)


Dockerized FastAPI wrapper for Kokoro-82M text-to-speech model w/CPU ONNX and NVIDIA GPU PyTorch support, handling, and auto-stitching


### License

[Apache-2.0 license](https://github.com/remsky/Kokoro-FastAPI/blob/master/LICENSE)

[2.5k\\
stars](https://github.com/remsky/Kokoro-FastAPI/stargazers) [353\\
forks](https://github.com/remsky/Kokoro-FastAPI/forks) [Branches](https://github.com/remsky/Kokoro-FastAPI/branches) [Tags](https://github.com/remsky/Kokoro-FastAPI/tags) [Activity](https://github.com/remsky/Kokoro-FastAPI/activity)

[Star](https://github.com/login?return_to=%2Fremsky%2FKokoro-FastAPI)

[Notifications](https://github.com/login?return_to=%2Fremsky%2FKokoro-FastAPI) You must be signed in to change notification settings

# remsky/Kokoro-FastAPI

master

[**8** Branches](https://github.com/remsky/Kokoro-FastAPI/branches) [**13** Tags](https://github.com/remsky/Kokoro-FastAPI/tags)

[Go to Branches page](https://github.com/remsky/Kokoro-FastAPI/branches)[Go to Tags page](https://github.com/remsky/Kokoro-FastAPI/tags)

Go to file

Code

## Folders and files

| Name | Name | Last commit message | Last commit date |
| --- | --- | --- | --- |
| ## Latest commit<br>[![remsky](https://avatars.githubusercontent.com/u/25017870?v=4&size=40)](https://github.com/remsky)[remsky](https://github.com/remsky/Kokoro-FastAPI/commits?author=remsky)<br>[Merge pull request](https://github.com/remsky/Kokoro-FastAPI/commit/2c7e1e8c0bea243add560f58f5c93010d73b5ee7) [#291](https://github.com/remsky/Kokoro-FastAPI/pull/291) [from RigleGit/patch-1](https://github.com/remsky/Kokoro-FastAPI/commit/2c7e1e8c0bea243add560f58f5c93010d73b5ee7)<br>Apr 22, 2025<br>[2c7e1e8](https://github.com/remsky/Kokoro-FastAPI/commit/2c7e1e8c0bea243add560f58f5c93010d73b5ee7) · Apr 22, 2025<br>## History<br>[396 Commits](https://github.com/remsky/Kokoro-FastAPI/commits/master/) |
| [.github](https://github.com/remsky/Kokoro-FastAPI/tree/master/.github ".github") | [.github](https://github.com/remsky/Kokoro-FastAPI/tree/master/.github ".github") | [Add tag existence check in release workflow](https://github.com/remsky/Kokoro-FastAPI/commit/10f240daadfe86041c84153b7a8eae0c46e8f6af "Add tag existence check in release workflow") | Apr 4, 2025 |
| [api](https://github.com/remsky/Kokoro-FastAPI/tree/master/api "api") | [api](https://github.com/remsky/Kokoro-FastAPI/tree/master/api "api") | [Apply suggestions from copilot](https://github.com/remsky/Kokoro-FastAPI/commit/d004b6d304a7bc0fd06133d33e84616ab057dd1c "Apply suggestions from copilot  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>") | Apr 4, 2025 |
| [assets](https://github.com/remsky/Kokoro-FastAPI/tree/master/assets "assets") | [assets](https://github.com/remsky/Kokoro-FastAPI/tree/master/assets "assets") | [Bump version to v0.2.0-pre, enhance Docker configurations for GPU sup…](https://github.com/remsky/Kokoro-FastAPI/commit/d452a6e114fc346c82ca600115961855be8ac243 "Bump version to v0.2.0-pre, enhance Docker configurations for GPU support, and refine text processing settings") | Feb 5, 2025 |
| [charts/kokoro-fastapi](https://github.com/remsky/Kokoro-FastAPI/tree/master/charts/kokoro-fastapi "This path skips through empty directories") | [charts/kokoro-fastapi](https://github.com/remsky/Kokoro-FastAPI/tree/master/charts/kokoro-fastapi "This path skips through empty directories") | [Bump version to 0.3.0 and update related configurations; add misaki p…](https://github.com/remsky/Kokoro-FastAPI/commit/6a2d3a54cf9adacebccfdec4ebe294974694146c "Bump version to 0.3.0 and update related configurations; add misaki patch script and remove obsolete build workflow") | Apr 4, 2025 |
| [dev](https://github.com/remsky/Kokoro-FastAPI/tree/master/dev "dev") | [dev](https://github.com/remsky/Kokoro-FastAPI/tree/master/dev "dev") | [CONTRIBUTING + Ruff format](https://github.com/remsky/Kokoro-FastAPI/commit/afa879546c1dd10e28ab3bd7e8dc9a3062d0ae99 "CONTRIBUTING + Ruff format") | Apr 4, 2025 |
| [docker](https://github.com/remsky/Kokoro-FastAPI/tree/master/docker "docker") | [docker](https://github.com/remsky/Kokoro-FastAPI/tree/master/docker "docker") | [Update Dockerfile to install Rust](https://github.com/remsky/Kokoro-FastAPI/commit/83325b749da4b1bfb9c44ce088515be5d597a828 "Update Dockerfile to install Rust  Rust is required to build sudachipy and pyopenjtalk-plus") | Apr 17, 2025 |
| [docs](https://github.com/remsky/Kokoro-FastAPI/tree/master/docs "docs") | [docs](https://github.com/remsky/Kokoro-FastAPI/tree/master/docs "docs") | [espeak-loader broken link fix, invalid pipeline state](https://github.com/remsky/Kokoro-FastAPI/commit/af0e6dad6e07a69a0ec399684177fa38ae074e22 "espeak-loader broken link fix, invalid pipeline state") | Feb 8, 2025 |
| [examples](https://github.com/remsky/Kokoro-FastAPI/tree/master/examples "examples") | [examples](https://github.com/remsky/Kokoro-FastAPI/tree/master/examples "examples") | [Minor change to trigger new release](https://github.com/remsky/Kokoro-FastAPI/commit/d826de87edab7de59144967d86048d235ac470ed "Minor change to trigger new release") | Apr 4, 2025 |
| [scripts](https://github.com/remsky/Kokoro-FastAPI/tree/master/scripts "scripts") | [scripts](https://github.com/remsky/Kokoro-FastAPI/tree/master/scripts "scripts") | [CONTRIBUTING + Ruff format](https://github.com/remsky/Kokoro-FastAPI/commit/afa879546c1dd10e28ab3bd7e8dc9a3062d0ae99 "CONTRIBUTING + Ruff format") | Apr 4, 2025 |
| [ui](https://github.com/remsky/Kokoro-FastAPI/tree/master/ui "ui") | [ui](https://github.com/remsky/Kokoro-FastAPI/tree/master/ui "ui") | [CONTRIBUTING + Ruff format](https://github.com/remsky/Kokoro-FastAPI/commit/afa879546c1dd10e28ab3bd7e8dc9a3062d0ae99 "CONTRIBUTING + Ruff format") | Apr 4, 2025 |
| [web](https://github.com/remsky/Kokoro-FastAPI/tree/master/web "web") | [web](https://github.com/remsky/Kokoro-FastAPI/tree/master/web "web") | [use local js file instead of the unpkg cdn](https://github.com/remsky/Kokoro-FastAPI/commit/2f420daad57f7563b743b4bbe38b3d0cddc4c53e "use local js file instead of the unpkg cdn  unpg.com was down, which broke the ui. I guess for stability deps should be local or at least pinned to a version.") | Mar 15, 2025 |
| [.coveragerc](https://github.com/remsky/Kokoro-FastAPI/blob/master/.coveragerc ".coveragerc") | [.coveragerc](https://github.com/remsky/Kokoro-FastAPI/blob/master/.coveragerc ".coveragerc") | [ci: enhance local saving feature, update voice selection to support m…](https://github.com/remsky/Kokoro-FastAPI/commit/5cc3bacac1a71ff9470305006aa766a40ab70d1d "ci: enhance local saving feature, update voice selection to support multiple voices, and improve output filename generation") | Jan 14, 2025 |
| [.dockerignore](https://github.com/remsky/Kokoro-FastAPI/blob/master/.dockerignore ".dockerignore") | [.dockerignore](https://github.com/remsky/Kokoro-FastAPI/blob/master/.dockerignore ".dockerignore") | [Update .gitignore to include additional patterns for Python, environm…](https://github.com/remsky/Kokoro-FastAPI/commit/5e9a0ae2b1fdbc9cc706412e1f4fcd97af400972 "Update .gitignore to include additional patterns for Python, environment, IDE, and project-specific files") | Jan 12, 2025 |
| [.gitattributes](https://github.com/remsky/Kokoro-FastAPI/blob/master/.gitattributes ".gitattributes") | [.gitattributes](https://github.com/remsky/Kokoro-FastAPI/blob/master/.gitattributes ".gitattributes") | [Add a .gitattributes](https://github.com/remsky/Kokoro-FastAPI/commit/7f15ba8fed1d69b4902f390bcead918bf8742960 "Add a .gitattributes") | Feb 18, 2025 |
| [.gitignore](https://github.com/remsky/Kokoro-FastAPI/blob/master/.gitignore ".gitignore") | [.gitignore](https://github.com/remsky/Kokoro-FastAPI/blob/master/.gitignore ".gitignore") | [added support for mps on mac with apple silicon](https://github.com/remsky/Kokoro-FastAPI/commit/9a9bc4aca91ff876a8711d73ea174a72c86c8ced "added support for mps on mac with apple silicon") | Mar 9, 2025 |
| [.python-version](https://github.com/remsky/Kokoro-FastAPI/blob/master/.python-version ".python-version") | [.python-version](https://github.com/remsky/Kokoro-FastAPI/blob/master/.python-version ".python-version") | [Initial swap to UV dependency management](https://github.com/remsky/Kokoro-FastAPI/commit/38e0b87320852ca627a988a6aca69286f4b1b8ef "Initial swap to UV dependency management") | Jan 11, 2025 |
| [.ruff.toml](https://github.com/remsky/Kokoro-FastAPI/blob/master/.ruff.toml ".ruff.toml") | [.ruff.toml](https://github.com/remsky/Kokoro-FastAPI/blob/master/.ruff.toml ".ruff.toml") | [ci: update dependency installation command to run 'uv sync' from the …](https://github.com/remsky/Kokoro-FastAPI/commit/36f85638acc85d9c7262f53a5503af2aa3ddf61a "ci: update dependency installation command to run 'uv sync' from the root directory") | Jan 13, 2025 |
| [CHANGELOG.md](https://github.com/remsky/Kokoro-FastAPI/blob/master/CHANGELOG.md "CHANGELOG.md") | [CHANGELOG.md](https://github.com/remsky/Kokoro-FastAPI/blob/master/CHANGELOG.md "CHANGELOG.md") | [Update CHANGELOG.md for version 0.3.0: add new features, changes, fix…](https://github.com/remsky/Kokoro-FastAPI/commit/ba796ef00370075d0a049a3508ae693529baeb42 "Update CHANGELOG.md for version 0.3.0: add new features, changes, fixes, and removals") | Apr 4, 2025 |
| [CONTRIBUTING.md](https://github.com/remsky/Kokoro-FastAPI/blob/master/CONTRIBUTING.md "CONTRIBUTING.md") | [CONTRIBUTING.md](https://github.com/remsky/Kokoro-FastAPI/blob/master/CONTRIBUTING.md "CONTRIBUTING.md") | [Adjust CONTRIBUTING.md, readme docker information and notes](https://github.com/remsky/Kokoro-FastAPI/commit/10caafe3fbb7b34383218e4e10e085cf79b0aec8 "Adjust CONTRIBUTING.md, readme docker information and notes") | Apr 4, 2025 |
| [LICENSE](https://github.com/remsky/Kokoro-FastAPI/blob/master/LICENSE "LICENSE") | [LICENSE](https://github.com/remsky/Kokoro-FastAPI/blob/master/LICENSE "LICENSE") | [Create LICENSE](https://github.com/remsky/Kokoro-FastAPI/commit/9b76ce2071c247faea43d57ce8135c87a2cd4156 "Create LICENSE") | Feb 9, 2025 |
| [README.md](https://github.com/remsky/Kokoro-FastAPI/blob/master/README.md "README.md") | [README.md](https://github.com/remsky/Kokoro-FastAPI/blob/master/README.md "README.md") | [Update README.md](https://github.com/remsky/Kokoro-FastAPI/commit/f1fa3404940e1422010b9be19f9b4996619e5e57 "Update README.md") | Apr 5, 2025 |
| [VERSION](https://github.com/remsky/Kokoro-FastAPI/blob/master/VERSION "VERSION") | [VERSION](https://github.com/remsky/Kokoro-FastAPI/blob/master/VERSION "VERSION") | [Bump version to 0.3.0 and update related configurations; add misaki p…](https://github.com/remsky/Kokoro-FastAPI/commit/6a2d3a54cf9adacebccfdec4ebe294974694146c "Bump version to 0.3.0 and update related configurations; add misaki patch script and remove obsolete build workflow") | Apr 4, 2025 |
| [debug.http](https://github.com/remsky/Kokoro-FastAPI/blob/master/debug.http "debug.http") | [debug.http](https://github.com/remsky/Kokoro-FastAPI/blob/master/debug.http "debug.http") | [Add model listing and retrieval endpoints with tests](https://github.com/remsky/Kokoro-FastAPI/commit/8ed2f2afb6537084f51f68bd8fc6362a5079fad3 "Add model listing and retrieval endpoints with tests") | Feb 9, 2025 |
| [docker-bake.hcl](https://github.com/remsky/Kokoro-FastAPI/blob/master/docker-bake.hcl "docker-bake.hcl") | [docker-bake.hcl](https://github.com/remsky/Kokoro-FastAPI/blob/master/docker-bake.hcl "docker-bake.hcl") | [Update Docker workflows to ignore markdown files and documentation in…](https://github.com/remsky/Kokoro-FastAPI/commit/1f354092fe4706aaf0847b67dc5c8ce1265d37cb "Update Docker workflows to ignore markdown files and documentation in publish triggers") | Feb 6, 2025 |
| [githubbanner.png](https://github.com/remsky/Kokoro-FastAPI/blob/master/githubbanner.png "githubbanner.png") | [githubbanner.png](https://github.com/remsky/Kokoro-FastAPI/blob/master/githubbanner.png "githubbanner.png") | [Update README with performance benchmarks and usage examples; add ben…](https://github.com/remsky/Kokoro-FastAPI/commit/aa2df458581c79b6cab34305f777611ce257906b "Update README with performance benchmarks and usage examples; add benchmark plotting script") | Dec 30, 2024 |
| [pyproject.toml](https://github.com/remsky/Kokoro-FastAPI/blob/master/pyproject.toml "pyproject.toml") | [pyproject.toml](https://github.com/remsky/Kokoro-FastAPI/blob/master/pyproject.toml "pyproject.toml") | [Bump version to 0.3.0 and update related configurations; add misaki p…](https://github.com/remsky/Kokoro-FastAPI/commit/6a2d3a54cf9adacebccfdec4ebe294974694146c "Bump version to 0.3.0 and update related configurations; add misaki patch script and remove obsolete build workflow") | Apr 4, 2025 |
| [pytest.ini](https://github.com/remsky/Kokoro-FastAPI/blob/master/pytest.ini "pytest.ini") | [pytest.ini](https://github.com/remsky/Kokoro-FastAPI/blob/master/pytest.ini "pytest.ini") | [Refactor Docker configurations for GPU and CPU, update test paths, an…](https://github.com/remsky/Kokoro-FastAPI/commit/ac7947b51aa3c4a01cc7a7b8132caddb2957c2e4 "Refactor Docker configurations for GPU and CPU, update test paths, and remove deprecated tests") | Feb 6, 2025 |
| [start-cpu.ps1](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-cpu.ps1 "start-cpu.ps1") | [start-cpu.ps1](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-cpu.ps1 "start-cpu.ps1") | [Fixed the ps1 files downloading packages before the venv and made all…](https://github.com/remsky/Kokoro-FastAPI/commit/22185dbc8971137035990814b5296f80ca8f217b "Fixed the ps1 files downloading packages before the venv and made all starrt scripts auto download models") | Mar 15, 2025 |
| [start-cpu.sh](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-cpu.sh "start-cpu.sh") | [start-cpu.sh](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-cpu.sh "start-cpu.sh") | [Bump version to 0.3.0 and update related configurations; add misaki p…](https://github.com/remsky/Kokoro-FastAPI/commit/6a2d3a54cf9adacebccfdec4ebe294974694146c "Bump version to 0.3.0 and update related configurations; add misaki patch script and remove obsolete build workflow") | Apr 4, 2025 |
| [start-gpu.ps1](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-gpu.ps1 "start-gpu.ps1") | [start-gpu.ps1](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-gpu.ps1 "start-gpu.ps1") | [Fixed the ps1 files downloading packages before the venv and made all…](https://github.com/remsky/Kokoro-FastAPI/commit/22185dbc8971137035990814b5296f80ca8f217b "Fixed the ps1 files downloading packages before the venv and made all starrt scripts auto download models") | Mar 15, 2025 |
| [start-gpu.sh](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-gpu.sh "start-gpu.sh") | [start-gpu.sh](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-gpu.sh "start-gpu.sh") | [Fixed the ps1 files downloading packages before the venv and made all…](https://github.com/remsky/Kokoro-FastAPI/commit/22185dbc8971137035990814b5296f80ca8f217b "Fixed the ps1 files downloading packages before the venv and made all starrt scripts auto download models") | Mar 15, 2025 |
| [start-gpu\_mac.sh](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-gpu_mac.sh "start-gpu_mac.sh") | [start-gpu\_mac.sh](https://github.com/remsky/Kokoro-FastAPI/blob/master/start-gpu_mac.sh "start-gpu_mac.sh") | [removed duplicated env and align with other shell scripts](https://github.com/remsky/Kokoro-FastAPI/commit/64ced408b7de079df5c8cdc23604f47b82e9ddca "removed duplicated env and align with other shell scripts") | Mar 30, 2025 |
| View all files |

## Repository files navigation

[![Kokoro TTS Banner](https://github.com/remsky/Kokoro-FastAPI/raw/master/githubbanner.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/githubbanner.png)

# _`FastKoko`_

[Permalink: FastKoko](https://github.com/remsky/kokoro-fastapi#fastkoko-)

[![Tests](https://camo.githubusercontent.com/acb557487b2e52586a61d1fc8d50e5f1ee304e47c2d35e81287a11e9bb00f010/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f74657374732d36392d6461726b677265656e)](https://github.com/remsky/Kokoro-FastAPI/blob/master)[![Coverage](https://camo.githubusercontent.com/b6725f201ebbf0602d0b4149e9a038bfe95b7c32c3758d7774172896e1afe28c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f7665726167652d35342532352d74616e)](https://github.com/remsky/Kokoro-FastAPI/blob/master)[![Try on Spaces](https://camo.githubusercontent.com/7209c5fad24fdf1f72c9e2609db7a6feec3713b8e724f041c8e3571110b08970/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372532305472792532306f6e2d5370616365732d626c7565)](https://huggingface.co/spaces/Remsky/Kokoro-TTS-Zero)

[![Kokoro](https://camo.githubusercontent.com/1831a543a9a2761d0c1e0a4bc8801966ea5ba28584b7ad1f3add265552f13496/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6b6f6b6f726f2d302e392e322d424235343230)](https://github.com/hexgrad/kokoro)[![Misaki](https://camo.githubusercontent.com/54760d54fc2543f55d3343015257afe8fc571dd866dc3db6a2df6d8e7984d653/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d6973616b692d302e392e332d423838363042)](https://github.com/hexgrad/misaki)

[![Tested at Model Commit](https://camo.githubusercontent.com/5e9b2a4b4f183e098f46ac2bc054a9320ae6cfc56db1aebb86fa446068d68414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6173742d2d7465737465642d2d6d6f64656c2d2d636f6d6d69742d312e303a3a393930316332622d626c7565)](https://huggingface.co/hexgrad/Kokoro-82M/commit/9901c2b79161b6e898b7ea857ae5298f47b8b0d6)

Dockerized FastAPI wrapper for [Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) text-to-speech model

- Multi-language support (English, Japanese, Korean, Chinese, _Vietnamese soon_)
- OpenAI-compatible Speech endpoint, NVIDIA GPU accelerated or CPU inference with PyTorch
- ONNX support coming soon, see v0.1.5 and earlier for legacy ONNX support in the interim
- Debug endpoints for monitoring system stats, integrated web UI on localhost:8880/web
- Phoneme-based audio generation, phoneme generation
- Per-word timestamped caption generation
- Voice mixing with weighted combinations

### Integration Guides

[Permalink: Integration Guides](https://github.com/remsky/kokoro-fastapi#integration-guides)

[![Helm Chart](https://camo.githubusercontent.com/43381c6412a4640340ab52b462b3031fa81ed01057727bcce66ec5f99849eda3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48656c6d25323043686172742d626c61636b3f7374796c653d666c6174266c6f676f3d68656c6d266c6f676f436f6c6f723d7768697465)](https://github.com/remsky/Kokoro-FastAPI/wiki/Setup-Kubernetes)[![DigitalOcean](https://camo.githubusercontent.com/e111092243cb0d3f56fc46338d97abeec925ee869c510bc0ba61a560fe0f2bd5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4469676974616c4f6365616e2d626c61636b3f7374796c653d666c6174266c6f676f3d6469676974616c6f6365616e266c6f676f436f6c6f723d7768697465)](https://github.com/remsky/Kokoro-FastAPI/wiki/Integrations-DigitalOcean)[![SillyTavern](https://camo.githubusercontent.com/db3de2e364e6018bb84456b90b1cb20aed898bf85fecc3ca8ad4bd747ee781c4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53696c6c7954617665726e2d626c61636b3f7374796c653d666c617426636f6c6f723d726564)](https://github.com/remsky/Kokoro-FastAPI/wiki/Integrations-SillyTavern)[![OpenWebUI](https://camo.githubusercontent.com/f4f53654410e8386f2ceb693bfc8c07009d04bc8b715f6609c06a05c929647fe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f70656e57656255492d626c61636b3f7374796c653d666c617426636f6c6f723d7768697465)](https://github.com/remsky/Kokoro-FastAPI/wiki/Integrations-OpenWebUi)

## Get Started

[Permalink: Get Started](https://github.com/remsky/kokoro-fastapi#get-started)

Quickest Start (docker run)

Pre built images are available to run, with arm/multi-arch support, and baked in models
Refer to the core/config.py file for a full list of variables which can be managed via the environment

```
# the `latest` tag can be used, though it may have some unexpected bonus features which impact stability.
 Named versions should be pinned for your regular usage.
 Feedback/testing is always welcome

docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest # CPU, or:
docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest  #NVIDIA GPU
```

Quick Start (docker compose)

1. Install prerequisites, and start the service using Docker Compose (Full setup including UI):
   - Install [Docker](https://www.docker.com/products/docker-desktop/)
   - Clone the repository:



     ```
     git clone https://github.com/remsky/Kokoro-FastAPI.git
     cd Kokoro-FastAPI

     cd docker/gpu  # For GPU support
     # or cd docker/cpu  # For CPU support
     docker compose up --build

     # *Note for Apple Silicon (M1/M2) users:
     # The current GPU build relies on CUDA, which is not supported on Apple Silicon.
     # If you are on an M1/M2/M3 Mac, please use the `docker/cpu` setup.
     # MPS (Apple's GPU acceleration) support is planned but not yet available.

     # Models will auto-download, but if needed you can manually download:
     python docker/scripts/download_model.py --output api/src/models/v1_0

     # Or run directly via UV:
     ./start-gpu.sh  # For GPU support
     ./start-cpu.sh  # For CPU support
     ```

Direct Run (via uv)

1. Install prerequisites ():
   - Install [astral-uv](https://docs.astral.sh/uv/)

   - Install [espeak-ng](https://github.com/espeak-ng/espeak-ng) in your system if you want it available as a fallback for unknown words/sounds. The upstream libraries may attempt to handle this, but results have varied.

   - Clone the repository:



     ```
     git clone https://github.com/remsky/Kokoro-FastAPI.git
     cd Kokoro-FastAPI
     ```







     Run the [model download script](https://github.com/remsky/Kokoro-FastAPI/blob/master/docker/scripts/download_model.py) if you haven't already

     Start directly via UV (with hot-reload)

     Linux and macOS



     ```
     ./start-cpu.sh OR
     ./start-gpu.sh
     ```







     Windows



     ```
     .\start-cpu.ps1 OR
     .\start-gpu.ps1
     ```

Up and Running?

Run locally as an OpenAI-Compatible Speech Endpoint

```
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8880/v1", api_key="not-needed"
)

with client.audio.speech.with_streaming_response.create(
    model="kokoro",
    voice="af_sky+af_bella", #single or multiple voicepack combo
    input="Hello world!"
  ) as response:
      response.stream_to_file("output.mp3")
```

- The API will be available at [http://localhost:8880](http://localhost:8880/)

- API Documentation: [http://localhost:8880/docs](http://localhost:8880/docs)

- Web Interface: [http://localhost:8880/web](http://localhost:8880/web)


[![API Documentation](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/docs-screenshot.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/docs-screenshot.png)[![Web UI Screenshot](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/webui-screenshot.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/webui-screenshot.png)

## Features

[Permalink: Features](https://github.com/remsky/kokoro-fastapi#features)

OpenAI-Compatible Speech Endpoint

```
# Using OpenAI's Python library
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8880/v1", api_key="not-needed")
response = client.audio.speech.create(
    model="kokoro",
    voice="af_bella+af_sky", # see /api/src/core/openai_mappings.json to customize
    input="Hello world!",
    response_format="mp3"
)

response.stream_to_file("output.mp3")
```

Or Via Requests:

```
import requests

response = requests.get("http://localhost:8880/v1/audio/voices")
voices = response.json()["voices"]

# Generate audio
response = requests.post(
    "http://localhost:8880/v1/audio/speech",
    json={
        "model": "kokoro",
        "input": "Hello world!",
        "voice": "af_bella",
        "response_format": "mp3",  # Supported: mp3, wav, opus, flac
        "speed": 1.0
    }
)

# Save audio
with open("output.mp3", "wb") as f:
    f.write(response.content)
```

Quick tests (run from another terminal):

```
python examples/assorted_checks/test_openai/test_openai_tts.py # Test OpenAI Compatibility
python examples/assorted_checks/test_voices/test_all_voices.py # Test all available voices
```

Voice Combination

- Weighted voice combinations using ratios (e.g., "af\_bella(2)+af\_heart(1)" for 67%/33% mix)
- Ratios are automatically normalized to sum to 100%
- Available through any endpoint by adding weights in parentheses
- Saves generated voicepacks for future use

Combine voices and generate audio:

```
import requests
response = requests.get("http://localhost:8880/v1/audio/voices")
voices = response.json()["voices"]

# Example 1: Simple voice combination (50%/50% mix)
response = requests.post(
    "http://localhost:8880/v1/audio/speech",
    json={
        "input": "Hello world!",
        "voice": "af_bella+af_sky",  # Equal weights
        "response_format": "mp3"
    }
)

# Example 2: Weighted voice combination (67%/33% mix)
response = requests.post(
    "http://localhost:8880/v1/audio/speech",
    json={
        "input": "Hello world!",
        "voice": "af_bella(2)+af_sky(1)",  # 2:1 ratio = 67%/33%
        "response_format": "mp3"
    }
)

# Example 3: Download combined voice as .pt file
response = requests.post(
    "http://localhost:8880/v1/audio/voices/combine",
    json="af_bella(2)+af_sky(1)"  # 2:1 ratio = 67%/33%
)

# Save the .pt file
with open("combined_voice.pt", "wb") as f:
    f.write(response.content)

# Use the downloaded voice file
response = requests.post(
    "http://localhost:8880/v1/audio/speech",
    json={
        "input": "Hello world!",
        "voice": "combined_voice",  # Use the saved voice file
        "response_format": "mp3"
    }
)
```

[![Voice Analysis Comparison](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/voice_analysis.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/voice_analysis.png)

Multiple Output Audio Formats

- mp3
- wav
- opus
- flac
- m4a
- pcm

[![Audio Format Comparison](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/format_comparison.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/format_comparison.png)

Streaming Support

```
# OpenAI-compatible streaming
from openai import OpenAI
client = OpenAI(
    base_url="http://localhost:8880/v1", api_key="not-needed")

# Stream to file
with client.audio.speech.with_streaming_response.create(
    model="kokoro",
    voice="af_bella",
    input="Hello world!"
) as response:
    response.stream_to_file("output.mp3")

# Stream to speakers (requires PyAudio)
import pyaudio
player = pyaudio.PyAudio().open(
    format=pyaudio.paInt16,
    channels=1,
    rate=24000,
    output=True
)

with client.audio.speech.with_streaming_response.create(
    model="kokoro",
    voice="af_bella",
    response_format="pcm",
    input="Hello world!"
) as response:
    for chunk in response.iter_bytes(chunk_size=1024):
        player.write(chunk)
```

Or via requests:

```
import requests

response = requests.post(
    "http://localhost:8880/v1/audio/speech",
    json={
        "input": "Hello world!",
        "voice": "af_bella",
        "response_format": "pcm"
    },
    stream=True
)

for chunk in response.iter_content(chunk_size=1024):
    if chunk:
        # Process streaming chunks
        pass
```

[![GPU First Token Timeline](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/gpu_first_token_timeline_openai.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/gpu_first_token_timeline_openai.png)[![CPU First Token Timeline](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/cpu_first_token_timeline_stream_openai.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/cpu_first_token_timeline_stream_openai.png)

Key Streaming Metrics:

- First token latency @ chunksize
  - ~300ms (GPU) @ 400
  - ~3500ms (CPU) @ 200 (older i7)
  - ~<1s (CPU) @ 200 (M3 Pro)
- Adjustable chunking settings for real-time playback

_Note: Artifacts in intonation can increase with smaller chunks_

## Processing Details

[Permalink: Processing Details](https://github.com/remsky/kokoro-fastapi#processing-details)

Performance Benchmarks

Benchmarking was performed on generation via the local API using text lengths up to feature-length books (~1.5 hours output), measuring processing time and realtime factor. Tests were run on:

- Windows 11 Home w/ WSL2
- NVIDIA 4060Ti 16gb GPU @ CUDA 12.1
- 11th Gen i7-11700 @ 2.5GHz
- 64gb RAM
- WAV native output
- H.G. Wells - The Time Machine (full text)

[![Processing Time](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/gpu_processing_time.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/gpu_processing_time.png)[![Realtime Factor](https://github.com/remsky/Kokoro-FastAPI/raw/master/assets/gpu_realtime_factor.png)](https://github.com/remsky/Kokoro-FastAPI/blob/master/assets/gpu_realtime_factor.png)

Key Performance Metrics:

- Realtime Speed: Ranges between 35x-100x (generation time to output audio length)
- Average Processing Rate: 137.67 tokens/second (cl100k\_base)

GPU Vs. CPU

```
# GPU: Requires NVIDIA GPU with CUDA 12.8 support (~35x-100x realtime speed)
cd docker/gpu
docker compose up --build

# CPU: PyTorch CPU inference
cd docker/cpu
docker compose up --build

```

_Note: Overall speed may have reduced somewhat with the structural changes to accommodate streaming. Looking into it_

Natural Boundary Detection

- Automatically splits and stitches at sentence boundaries
- Helps to reduce artifacts and allow long form processing as the base model is only currently configured for approximately 30s output

The model is capable of processing up to a 510 phonemized token chunk at a time, however, this can often lead to 'rushed' speech or other artifacts. An additional layer of chunking is applied in the server, that creates flexible chunks with a `TARGET_MIN_TOKENS` , `TARGET_MAX_TOKENS`, and `ABSOLUTE_MAX_TOKENS` which are configurable via environment variables, and set to 175, 250, 450 by default

Timestamped Captions & Phonemes

Generate audio with word-level timestamps without streaming:

```
import requests
import base64
import json

response = requests.post(
    "http://localhost:8880/dev/captioned_speech",
    json={
        "model": "kokoro",
        "input": "Hello world!",
        "voice": "af_bella",
        "speed": 1.0,
        "response_format": "mp3",
        "stream": False,
    },
    stream=False
)

with open("output.mp3","wb") as f:

    audio_json=json.loads(response.content)

    # Decode base 64 stream to bytes
    chunk_audio=base64.b64decode(audio_json["audio"].encode("utf-8"))

    # Process streaming chunks
    f.write(chunk_audio)

    # Print word level timestamps
    print(audio_json["timestamps"])
```

Generate audio with word-level timestamps with streaming:

```
import requests
import base64
import json

response = requests.post(
    "http://localhost:8880/dev/captioned_speech",
    json={
        "model": "kokoro",
        "input": "Hello world!",
        "voice": "af_bella",
        "speed": 1.0,
        "response_format": "mp3",
        "stream": True,
    },
    stream=True
)

f=open("output.mp3","wb")
for chunk in response.iter_lines(decode_unicode=True):
    if chunk:
        chunk_json=json.loads(chunk)

        # Decode base 64 stream to bytes
        chunk_audio=base64.b64decode(chunk_json["audio"].encode("utf-8"))

        # Process streaming chunks
        f.write(chunk_audio)

        # Print word level timestamps
        print(chunk_json["timestamps"])
```

Phoneme & Token Routes

Convert text to phonemes and/or generate audio directly from phonemes:

```
import requests

def get_phonemes(text: str, language: str = "a"):
    """Get phonemes and tokens for input text"""
    response = requests.post(
        "http://localhost:8880/dev/phonemize",
        json={"text": text, "language": language}  # "a" for American English
    )
    response.raise_for_status()
    result = response.json()
    return result["phonemes"], result["tokens"]

def generate_audio_from_phonemes(phonemes: str, voice: str = "af_bella"):
    """Generate audio from phonemes"""
    response = requests.post(
        "http://localhost:8880/dev/generate_from_phonemes",
        json={"phonemes": phonemes, "voice": voice},
        headers={"Accept": "audio/wav"}
    )
    if response.status_code != 200:
        print(f"Error: {response.text}")
        return None
    return response.content

# Example usage
text = "Hello world!"
try:
    # Convert text to phonemes
    phonemes, tokens = get_phonemes(text)
    print(f"Phonemes: {phonemes}")  # e.g. ðɪs ɪz ˈoʊnli ɐ tˈɛst
    print(f"Tokens: {tokens}")      # Token IDs including start/end tokens

    # Generate and save audio
    if audio_bytes := generate_audio_from_phonemes(phonemes):
        with open("speech.wav", "wb") as f:
            f.write(audio_bytes)
        print(f"Generated {len(audio_bytes)} bytes of audio")
except Exception as e:
    print(f"Error: {e}")
```

See `examples/phoneme_examples/generate_phonemes.py` for a sample script.

Debug Endpoints

Monitor system state and resource usage with these endpoints:

- `/debug/threads` \- Get thread information and stack traces
- `/debug/storage` \- Monitor temp file and output directory usage
- `/debug/system` \- Get system information (CPU, memory, GPU)
- `/debug/session_pools` \- View ONNX session and CUDA stream status

Useful for debugging resource exhaustion or performance issues.

## Known Issues & Troubleshooting

[Permalink: Known Issues & Troubleshooting](https://github.com/remsky/kokoro-fastapi#known-issues--troubleshooting)

Missing words & Missing some timestamps

The api will automaticly do text normalization on input text which may incorrectly remove or change some phrases. This can be disabled by adding `"normalization_options":{"normalize": false}` to your request json:

```
import requests

response = requests.post(
    "http://localhost:8880/v1/audio/speech",
    json={
        "input": "Hello world!",
        "voice": "af_heart",
        "response_format": "pcm",
        "normalization_options":
        {
            "normalize": False
        }
    },
    stream=True
)

for chunk in response.iter_content(chunk_size=1024):
    if chunk:
        # Process streaming chunks
        pass
```

Versioning & Development

**Branching Strategy:**

- **`release` branch:** Contains the latest stable build, recommended for production use. Docker images tagged with specific versions (e.g., `v0.3.0`) are built from this branch.
- **`master` branch:** Used for active development. It may contain experimental features, ongoing changes, or fixes not yet in a stable release. Use this branch if you want the absolute latest code, but be aware it might be less stable. The `latest` Docker tag often points to builds from this branch.

Note: This is a _development_ focused project at its core.

If you run into trouble, you may have to roll back a version on the release tags if something comes up, or build up from source and/or troubleshoot + submit a PR.

Free and open source is a community effort, and there's only really so many hours in a day. If you'd like to support the work, feel free to open a PR, buy me a coffee, or report any bugs/features/etc you find during use.

[![Buy Me A Coffee](https://camo.githubusercontent.com/8993389497c0b621c7eb91826bf805bf242c495053f2f39319595a6c53927c7c/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f76322f64656661756c742d76696f6c65742e706e67)](https://www.buymeacoffee.com/remsky)Linux GPU Permissions

Some Linux users may encounter GPU permission issues when running as non-root.
Can't guarantee anything, but here are some common solutions, consider your security requirements carefully

### Option 1: Container Groups (Likely the best option)

[Permalink: Option 1: Container Groups (Likely the best option)](https://github.com/remsky/kokoro-fastapi#option-1-container-groups-likely-the-best-option)

```
services:
  kokoro-tts:
    # ... existing config ...
    group_add:
      - "video"
      - "render"
```

### Option 2: Host System Groups

[Permalink: Option 2: Host System Groups](https://github.com/remsky/kokoro-fastapi#option-2-host-system-groups)

```
services:
  kokoro-tts:
    # ... existing config ...
    user: "${UID}:${GID}"
    group_add:
      - "video"
```

Note: May require adding host user to groups: `sudo usermod -aG docker,video $USER` and system restart.

### Option 3: Device Permissions (Use with caution)

[Permalink: Option 3: Device Permissions (Use with caution)](https://github.com/remsky/kokoro-fastapi#option-3-device-permissions-use-with-caution)

```
services:
  kokoro-tts:
    # ... existing config ...
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-uvm:/dev/nvidia-uvm
```

⚠️ Warning: Reduces system security. Use only in development environments.

Prerequisites: NVIDIA GPU, drivers, and container toolkit must be properly configured.

Visit [NVIDIA Container Toolkit installation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) for more detailed information

## Model and License

[Permalink: Model and License](https://github.com/remsky/kokoro-fastapi#model-and-license)

Model

This API uses the [Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) model from HuggingFace.

Visit the model page for more details about training, architecture, and capabilities. I have no affiliation with any of their work, and produced this wrapper for ease of use and personal projects.

License
This project is licensed under the Apache License 2.0 - see below for details:

- The Kokoro model weights are licensed under Apache 2.0 (see [model page](https://huggingface.co/hexgrad/Kokoro-82M))
- The FastAPI wrapper code in this repository is licensed under Apache 2.0 to match
- The inference code adapted from StyleTTS2 is MIT licensed

The full Apache 2.0 license text can be found at: [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)

## About

Dockerized FastAPI wrapper for Kokoro-82M text-to-speech model w/CPU ONNX and NVIDIA GPU PyTorch support, handling, and auto-stitching


### Topics

[pytorch](https://github.com/topics/pytorch "Topic: pytorch") [tts](https://github.com/topics/tts "Topic: tts") [uv](https://github.com/topics/uv "Topic: uv") [tts-api](https://github.com/topics/tts-api "Topic: tts-api") [onnx](https://github.com/topics/onnx "Topic: onnx") [kokoro](https://github.com/topics/kokoro "Topic: kokoro") [fastapi](https://github.com/topics/fastapi "Topic: fastapi") [onnxruntime](https://github.com/topics/onnxruntime "Topic: onnxruntime") [huggingface-spaces](https://github.com/topics/huggingface-spaces "Topic: huggingface-spaces") [sillytavern](https://github.com/topics/sillytavern "Topic: sillytavern") [openwebui](https://github.com/topics/openwebui "Topic: openwebui") [openai-compatible-api](https://github.com/topics/openai-compatible-api "Topic: openai-compatible-api") [kokoro-tts](https://github.com/topics/kokoro-tts "Topic: kokoro-tts")

### Resources

[Readme](https://github.com/remsky/kokoro-fastapi#readme-ov-file)

### License

[Apache-2.0 license](https://github.com/remsky/kokoro-fastapi#Apache-2.0-1-ov-file)

[Activity](https://github.com/remsky/Kokoro-FastAPI/activity)

### Stars

[**2.5k**\\
stars](https://github.com/remsky/Kokoro-FastAPI/stargazers)

### Watchers

[**28**\\
watching](https://github.com/remsky/Kokoro-FastAPI/watchers)

### Forks

[**353**\\
forks](https://github.com/remsky/Kokoro-FastAPI/forks)

[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fremsky%2FKokoro-FastAPI&report=remsky+%28user%29)

## [Releases\  13](https://github.com/remsky/Kokoro-FastAPI/releases)

[v0.2.3\\
Latest\\
\\
Mar 7, 2025](https://github.com/remsky/Kokoro-FastAPI/releases/tag/v0.2.3)

[\+ 12 releases](https://github.com/remsky/Kokoro-FastAPI/releases)

## Sponsor this project

- ![buy_me_a_coffee](https://github.githubassets.com/assets/buy_me_a_coffee-63ed78263f6e.svg)[buymeacoffee.com/ **remsky**](https://buymeacoffee.com/remsky)

## [Packages\  3](https://github.com/users/remsky/packages?repo_name=Kokoro-FastAPI)

- [kokoro-fastapi-cpu](https://github.com/users/remsky/packages/container/package/kokoro-fastapi-cpu)
- [kokoro-fastapi-gpu](https://github.com/users/remsky/packages/container/package/kokoro-fastapi-gpu)
- [kokoro-fastapi-ui](https://github.com/users/remsky/packages/container/package/kokoro-fastapi-ui)

## [Contributors\  19](https://github.com/remsky/Kokoro-FastAPI/graphs/contributors)

- [![@remsky](https://avatars.githubusercontent.com/u/25017870?s=64&v=4)](https://github.com/remsky)
- [![@fireblade2534](https://avatars.githubusercontent.com/u/77405729?s=64&v=4)](https://github.com/fireblade2534)
- [![@jteijema](https://avatars.githubusercontent.com/u/28191416?s=64&v=4)](https://github.com/jteijema)
- [![@rampadc](https://avatars.githubusercontent.com/u/5560025?s=64&v=4)](https://github.com/rampadc)
- [![@richardr1126](https://avatars.githubusercontent.com/u/29444239?s=64&v=4)](https://github.com/richardr1126)
- [![@dino65-dev](https://avatars.githubusercontent.com/u/131891590?s=64&v=4)](https://github.com/dino65-dev)
- [![@zucher](https://avatars.githubusercontent.com/u/3034596?s=64&v=4)](https://github.com/zucher)
- [![@eschmidbauer](https://avatars.githubusercontent.com/u/7139998?s=64&v=4)](https://github.com/eschmidbauer)
- [![@JoshRosen](https://avatars.githubusercontent.com/u/50748?s=64&v=4)](https://github.com/JoshRosen)
- [![@kimnzl](https://avatars.githubusercontent.com/u/567215?s=64&v=4)](https://github.com/kimnzl)
- [![@randombk](https://avatars.githubusercontent.com/u/1633923?s=64&v=4)](https://github.com/randombk)
- [![@mpnsk](https://avatars.githubusercontent.com/u/5862305?s=64&v=4)](https://github.com/mpnsk)
- [![@RigleGit](https://avatars.githubusercontent.com/u/8595185?s=64&v=4)](https://github.com/RigleGit)
- [![@Galunid](https://avatars.githubusercontent.com/u/10298730?s=64&v=4)](https://github.com/Galunid)

[\+ 5 contributors](https://github.com/remsky/Kokoro-FastAPI/graphs/contributors)

## Languages

- [Python74.9%](https://github.com/remsky/Kokoro-FastAPI/search?l=python)
- [JavaScript13.1%](https://github.com/remsky/Kokoro-FastAPI/search?l=javascript)
- [CSS7.2%](https://github.com/remsky/Kokoro-FastAPI/search?l=css)
- [HTML1.7%](https://github.com/remsky/Kokoro-FastAPI/search?l=html)
- [Shell1.2%](https://github.com/remsky/Kokoro-FastAPI/search?l=shell)
- [Dockerfile0.9%](https://github.com/remsky/Kokoro-FastAPI/search?l=dockerfile)
- Other1.0%

You can’t perform that action at this time.