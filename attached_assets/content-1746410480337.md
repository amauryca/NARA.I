Contents

- Introducing Multilingual
- Speaking Naturally
- Training Overview
- Finetuning Overview
- Pretraining Data
- Model Release Information
- Summary

## Orpheus Can Speak Any Language

April 9, 2025

Research Release for Fr, De, Es, It, Zh, Ko, Hi & a training manual for any language

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/canopyai/Orpheus-TTS)[![Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md.svg)](https://huggingface.co/canopylabs)[![Colab Notebook](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1KhXT56UePPUHhqitJNUxq63k-pQomz3N?usp=sharing)

Existing speech models struggle to speak in languages other than English as they are built on architectures that do not scale well to other languages, with low data. We present Orpheus Multilingual, a family of state-of-the-art speech-LLMs, for highly expressive speech, across multiple languages.

We have seen dozens, if not hundreds, of developers aim to create multilingual versions of Orpheus 0.1 for their own use cases, so we offer a detailed guide on multilingual training guidance, which is both straightforward and relatively inexpensive.

Your browser does not support the video tag.

Today we offer pretrained and finetuned models for the following languages: [French](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba), [German](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba), [Spanish](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba), [Italian](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba), [Chinese](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba), [Korean](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba), and [Hindi](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba).

We demonstrate extremely high quality, aesthetically pleasing, speech generation even given small amounts of multilingual speech data. All multilingual models follow the same architecture, and prompt format, as our pretrained models so can be used with any exisiting libraries.

Speaking Naturally

Models that aim to mimic human speech should be able to model:

- The appropriate emotional tonality given the content of the sentence (i.e. sound happy/sad etc)
- Produce human non-speech sounds like laughing, sighing, etc depending on the context.
- Make disfluencies, self-interruption, and other artefacts within sentences sound realistic.

| French | German | Mandarin |
| --- | --- | --- |
| Your browser does not support the audio element. | Your browser does not support the audio element. | Your browser does not support the audio element. |
| Your browser does not support the audio element. | Your browser does not support the audio element. | Your browser does not support the audio element. |
| Your browser does not support the audio element. | Your browser does not support the audio element. | Your browser does not support the audio element. |

Training Overview

We pretrain the model, then finetune it. All pretraining and finetuning is done on language specific data.

All weights were initialised from our existing base model Orpheus-3b-0.1-pretrained. We then extended training on this base model on language specific data. This data was in the form of text-speech pairs.

| language(s) | Quantity (1000 hrs) |
| --- | --- |
| French | 5 |
| German | 5 |
| Mandarin | 20 |
| Korean | 5 |
| Hindi | 1 |
| Spanish + Italian | 1 each |

Other than the number of machines we use the same hyperpararmeters as we do for finetuning. We concatenate input ids sequences together to lengths of 8192.

Below, we can see a typical loss curve for training on a new language. We see that the model starts at much better place on the loss curve, compared to initialising with random weights, if we use the Orpheus Pretrained as the base model. We also see that the model would most likely benefit more significantly more pretraining data.

![Architecture](https://canopylabs.ai/assets/images/de-pretraining.png)

Here are some suggestions for improving pretraining performance, compared to our method:

- More Pretraining Data The more pretraining data you have, the better the performance. We have seen that 5k hours of pretraining data is a good starting point, but English which has extremely high performance has 100k hours of pretraining data, and there is no reason to believe that this wouldn't benefit another language.
- Train on German Text Tokens We experienced a significant performance increase when we trained on English text tokens. We think boosting the models semantic understanding of German, perhaps by bridging it with English will offer better performance also.

Finetuning Overview

We pay professional actors to record exactly 300 lines each, aiming for at least 2 voice actors for each language. We finetune the appropriate newly extended pretrained models on the data collected by the voice actor. In the scripts given to the voice actors, we include emotional tags translated into the actor's language. There may be some inconsistency in tags because of interpretation by the voice actors and translations which do not fully capture the nuance of a tag. Here are the hyperpararmeters used for finetuning.

| Hyper Parameter | Value |
| --- | --- |
| Learning Rate | 5e-5 |
| Num Machines | 1 |
| Trainable Parameters | All |
| Learning Rate Schedule | cosine decay |
| Batch Size | 1 |
| Precision | bf16 |

We also demonstrate the importance of pretraining in a specific language below before finetuning through a simple ablation. Here we see the loss curves of the German voice actors' finetuning set when we train the base En model vs when we train the De Model (which has seen an additional ~ 5k hours of German).

![Architecture](https://canopylabs.ai/assets/images/de-tune-steps.png)

Similar to other languages we see a much better performance when finetuning on the model that has been been specifically pretrained on German than the English base model. The more hours of pretraining data in German, the better the performance.

We also finetune tags into the model for each language. Here is the frequency of the most prevalent tags in the Korean finetuning dataset.

| tag | frequency |
| --- | --- |
| í•œìˆ¨ | 93 |
| í— | 72 |
| í—›ê¸°ì¹¨ | 58 |
| í›Œì© | 57 |
| í•˜í’ˆ | 51 |
| ë‚„ë‚„ | 50 |
| ì‹ ìŒ | 25 |
| ì‘ì€ ì›ƒìŒ | 16 |
| ê¸°ì¹¨ | 14 |
| ìœ¼ë¥´ë  | 10 |
| í›Œì©ì„ | 8 |
| ì•½ê°„ ì›ƒìŒ | 7 |
| ìœ¼ë¥´ë ê±°ë¦¼ | 6 |
| í‚¥í‚¥ | 5 |

Understanding Pretraining Data

We have seen that the amount of pretraining data is very important for the performance of a model. Similarly, and more subtely is the quality of the pretraining data. Ideally, you want your finetuning data to be a "subset" of your pretraining data, and the finetuning stage should be thought of as teaching the model to selectively output part of its pretraining data.

Following this principle, we can see examples of bad pretraining data may be:

- Synthetically Generated Few Speaker As tempting as it is to use a large amount of synthetically generated data from one speaker, your model will catastrophically forget how to speak like another speaker.
- Wrong Sample Rate If your data is natively 16kHz, like a lot of speech data is, you can artificially upsample this to 24kHz. However, this is not the same as having natively 24kHz data. The model will learn to speak at 24k Hz, but it will not be able to produce the same quality of speech as a model trained on 24kHz data, before feeding into SNAC, however it is not "true" 24kHz data. Instead you are better off resampling your finetuning data down to 16kHz and then back up to 24kHz, before tokenization, as we do for Spanish and Italian.

Model Release Voices

Here is information about the finetuned versions of each base model. Voices are in order or quality.

| Language | Voices | Supported Tags |
| --- | --- | --- |
| French | pierre, amelie, marie | chuckle, cough, gasp, groan, laugh, sigh, sniffle, whimper, yawn |
| German | jana, thomas, max | chuckle, cough, gasp, groan, laugh, sigh, sniffle, yawn |
| Korean | ìœ ë‚˜, ì¤€ì„œ | í•œìˆ¨, í—, í—›ê¸°ì¹¨, í›Œì©, í•˜í’ˆ, ë‚„ë‚„, ì‹ ìŒ, ì‘ì€ ì›ƒìŒ, ê¸°ì¹¨, ìœ¼ë¥´ë  |
| Hindi | à¤‹à¤¤à¤¿à¤•à¤¾ (more coming) | coming soon |
| Mandarin | é•¿ä¹, ç™½èŠ· | å¬‰ç¬‘, è½»ç¬‘, å‘»åŸ, å¤§ç¬‘, å’³å—½, æŠ½é¼»å­, å’³ |
| Spanish | javi, sergio, maria | groan, chuckle, gasp, resoplido, laugh, yawn, cough |
| Italian | pietro, giulia, carlo | sigh, laugh, cough, sniffle, groan, yawn, gemito, gasp |

This is a research release so there is variance in performance of each of our models. We assess quality of a model by its stability, and expressiveness. Our license is as permissive as possible, so we are happy with commercial usage, but we reiterate that this is a research release, and these models are not designed to be used in production. All performance is relative

| Language | Pretrained | Finetuned |
| --- | --- | --- |
| French | ğŸ˜€ | ğŸ˜ |
| German | ğŸ˜€ | ğŸ˜ |
| Korean | ğŸ˜€ | ğŸ˜ |
| Hindi | ğŸ˜” | ğŸ˜” |
| Mandarin | ğŸ˜€ | ğŸ˜ |
| Spanish | ğŸ˜” | ğŸ˜” |
| Italian | ğŸ˜” | ğŸ˜” |

ğŸ˜€ ~ Highest Performance ğŸ˜ ~ Medium Performance ğŸ˜” ~ Lowest Performance

Conclusion

Orpheus provides a simpler, higher quality, and more customisable approach to creating speech models than all previous approaches. We have been inspired by the plethora of awesome finetunes of Orpheus 0.1, and the amazing work done by the community to create multilingual models independently. We hope these models, this guide, and our open tuning code, will provide the necessary toolkit for you to develop your own models. We are very active on Github and happy to help with any issues. If you need more support please reach out to us via email.

**We have no further immediate plans for working on multilingual models. We hope that our research models and guide serve as a foundation to train multilingual models for more languages and use cases.**

~ The Canopy Labs Team